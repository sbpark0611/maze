diff --git a/envs/memory_maze/maze.py b/envs/memory_maze/maze.py
index 5d26201..d73bf72 100644
--- a/envs/memory_maze/maze.py
+++ b/envs/memory_maze/maze.py
@@ -213,7 +213,6 @@ class MemoryMazeTask(random_goal_maze.NullGoalMaze):
         return True
 
     def _pick_new_target(self, rng: RandomState):
-        print("_pick_new_target")
         while True:
             ix = rng.randint(len(self._targets))
             # if self._targets[ix].activated:
diff --git a/envs/memory_maze/tasks.py b/envs/memory_maze/tasks.py
index c315cdf..bbbd58c 100644
--- a/envs/memory_maze/tasks.py
+++ b/envs/memory_maze/tasks.py
@@ -22,7 +22,7 @@ def memory_maze_9x9(**kwargs):
         roomMinSize = 3,
     }
     """
-    return _memory_maze(9, 2, 250, **kwargs) # Sangbin changed it from 250
+    return _memory_maze(9, 2, 250, **kwargs)
 
 
 def memory_maze_11x11(**kwargs):
diff --git a/envs/memory_maze/wrappers.py b/envs/memory_maze/wrappers.py
index 69bbb12..3bf298a 100644
--- a/envs/memory_maze/wrappers.py
+++ b/envs/memory_maze/wrappers.py
@@ -9,6 +9,8 @@ import sys
 sys.modules["gym"] = gym
 from math import inf
 import copy
+import cv2
+import time
 
 
 class Wrapper(dm_env.Environment):
@@ -257,6 +259,8 @@ class ObsWrapper(gym.Wrapper):
         self.actions_for_one_grid = 5
 
     def reset(self, seed = 0):
+        #print("start")
+        self.s = 0
         obs, info = self.env.reset(seed)
         self.prev_image = obs["image"]
         new_obs = {"image": obs["image"], "prev_action": 0, "prev_image": self.prev_image, "goal": obs["target_color"]}
@@ -267,12 +271,16 @@ class ObsWrapper(gym.Wrapper):
         return new_obs, info
 
     def step(self, action):
+        self.s += 1
+        #print("step",self.s,"action:",action)
         obs, reward, done, truncate, info = self.env.step(action)
 
         # add num of actions to get a target
         if reward > 0:
             self.oracle_min_num_actions += len(obs['path']) * self.actions_for_one_grid
 
+        cv2.imshow("train", obs["image"])
+        cv2.waitKey(1) 
         new_obs = {"image": obs["image"], "prev_action": action, "prev_image": self.prev_image, "goal": obs["target_color"]}
         self.prev_image = copy.deepcopy(obs["image"])
 
diff --git a/gui.py b/gui.py
index 45ad04c..d2a9d67 100644
--- a/gui.py
+++ b/gui.py
@@ -97,6 +97,7 @@ def main():
             assert 'image' in obs, 'Expecting dictionary observation with obs["image"]'
             image = obs['image']  # type: ignore
         else:
+            print(obs)
             assert isinstance(obs, np.ndarray) and len(obs.shape) == 3, 'Expecting image observation'
             image = obs
         image = Image.fromarray(image)
@@ -179,7 +180,7 @@ def main():
             print(f'reward: {reward}')
         if done or force_reset:
             print(f'Episode done - length: {steps}  return: {return_}')
-            obs = env.reset()
+            obs, _ = env.reset()
             steps = 0
             return_ = 0.0
             episode += 1
diff --git a/ppo/epn/epn.py b/ppo/epn/epn.py
index dbb50db..9ce7b2d 100644
--- a/ppo/epn/epn.py
+++ b/ppo/epn/epn.py
@@ -441,7 +441,6 @@ class EPNPPO(OnPolicyAlgorithm):
         for param in self.policy.model.image_embedding_conv.parameters():
             params.extend(param.cpu().detach().numpy().flatten())
 
-        print(np.mean(params), np.std(params))
         self.logger.record("train/embedding_weight_mean", np.mean(params))
         self.logger.record("train/embedding_weight_stddev", np.std(params))
 
