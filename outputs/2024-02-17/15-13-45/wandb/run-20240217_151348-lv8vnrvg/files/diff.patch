diff --git a/envs/memory_maze/maze.py b/envs/memory_maze/maze.py
index 5d26201..ed3eefc 100644
--- a/envs/memory_maze/maze.py
+++ b/envs/memory_maze/maze.py
@@ -213,11 +213,10 @@ class MemoryMazeTask(random_goal_maze.NullGoalMaze):
         return True
 
     def _pick_new_target(self, rng: RandomState):
-        print("_pick_new_target")
         while True:
             ix = rng.randint(len(self._targets))
-            # if self._targets[ix].activated:
-                # continue  # Skip the target that the agent is touching
+            if self._targets[ix].activated:
+                continue  # Skip the target that the agent is touching
             self._current_target_ix = ix
             break
 
diff --git a/envs/memory_maze/tasks.py b/envs/memory_maze/tasks.py
index c315cdf..bbbd58c 100644
--- a/envs/memory_maze/tasks.py
+++ b/envs/memory_maze/tasks.py
@@ -22,7 +22,7 @@ def memory_maze_9x9(**kwargs):
         roomMinSize = 3,
     }
     """
-    return _memory_maze(9, 2, 250, **kwargs) # Sangbin changed it from 250
+    return _memory_maze(9, 2, 250, **kwargs)
 
 
 def memory_maze_11x11(**kwargs):
diff --git a/envs/memory_maze/wrappers.py b/envs/memory_maze/wrappers.py
index 69bbb12..96734e6 100644
--- a/envs/memory_maze/wrappers.py
+++ b/envs/memory_maze/wrappers.py
@@ -9,6 +9,8 @@ import sys
 sys.modules["gym"] = gym
 from math import inf
 import copy
+import cv2
+import time
 
 
 class Wrapper(dm_env.Environment):
@@ -257,6 +259,8 @@ class ObsWrapper(gym.Wrapper):
         self.actions_for_one_grid = 5
 
     def reset(self, seed = 0):
+        #print("start")
+        self.s = 0
         obs, info = self.env.reset(seed)
         self.prev_image = obs["image"]
         new_obs = {"image": obs["image"], "prev_action": 0, "prev_image": self.prev_image, "goal": obs["target_color"]}
@@ -267,12 +271,17 @@ class ObsWrapper(gym.Wrapper):
         return new_obs, info
 
     def step(self, action):
+        self.s += 1
+        #print("step",self.s,"action:",action)
         obs, reward, done, truncate, info = self.env.step(action)
 
         # add num of actions to get a target
         if reward > 0:
+            print("YOU RECEIVED THE AMAZING REWARD!!!!!!!!!!!")
             self.oracle_min_num_actions += len(obs['path']) * self.actions_for_one_grid
 
+        cv2.imshow("train", cv2.resize(obs["image"], dsize=(720,720)))
+        cv2.waitKey(1) 
         new_obs = {"image": obs["image"], "prev_action": action, "prev_image": self.prev_image, "goal": obs["target_color"]}
         self.prev_image = copy.deepcopy(obs["image"])
 
diff --git a/gui.py b/gui.py
index 45ad04c..d2a9d67 100644
--- a/gui.py
+++ b/gui.py
@@ -97,6 +97,7 @@ def main():
             assert 'image' in obs, 'Expecting dictionary observation with obs["image"]'
             image = obs['image']  # type: ignore
         else:
+            print(obs)
             assert isinstance(obs, np.ndarray) and len(obs.shape) == 3, 'Expecting image observation'
             image = obs
         image = Image.fromarray(image)
@@ -179,7 +180,7 @@ def main():
             print(f'reward: {reward}')
         if done or force_reset:
             print(f'Episode done - length: {steps}  return: {return_}')
-            obs = env.reset()
+            obs, _ = env.reset()
             steps = 0
             return_ = 0.0
             episode += 1
diff --git a/ppo/epn/epn.py b/ppo/epn/epn.py
index dbb50db..cf4c6b5 100644
--- a/ppo/epn/epn.py
+++ b/ppo/epn/epn.py
@@ -441,7 +441,6 @@ class EPNPPO(OnPolicyAlgorithm):
         for param in self.policy.model.image_embedding_conv.parameters():
             params.extend(param.cpu().detach().numpy().flatten())
 
-        print(np.mean(params), np.std(params))
         self.logger.record("train/embedding_weight_mean", np.mean(params))
         self.logger.record("train/embedding_weight_stddev", np.std(params))
 
@@ -461,8 +460,6 @@ class EPNPPO(OnPolicyAlgorithm):
         )
         callback.on_training_start(locals(), globals())
 
-        max_rew_ep_info = 0
-
         while self.num_timesteps < total_timesteps:
             continue_training = self.collect_rollouts(
                 self.env,
@@ -488,8 +485,6 @@ class EPNPPO(OnPolicyAlgorithm):
                     avg_rew = safe_mean(
                         [ep_info["r"] for ep_info in self.ep_info_buffer]
                     )
-                    max_rew_ep_info = max(max_rew_ep_info, avg_rew)
-                    print(max_rew_ep_info)
                     self.logger.record(
                         "rollout/ep_rew_mean",
                         avg_rew,
